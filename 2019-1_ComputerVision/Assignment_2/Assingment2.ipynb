{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "PWbpgsGd-gyv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qkpywvtn-olJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "\"\"\"\n",
        "We will do the following steps in order:\n",
        "1. Load and normalizing the CIFAR10 training and test datasets using 'torchvision'\n",
        "2. Define a Convolutional Neural Network\n",
        "3. Define a loss function\n",
        "4. Train the network on the training data\n",
        "5. Test the network on the test data\n",
        "\"\"\"\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.nn.init as init\n",
        "import copy\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "############################################\n",
        "# 1. Loading and normalizing CIFAR-10\n",
        "############################################\n",
        "\n",
        "# The output of torchvision datasets are PILImage images of range [0, 1].\n",
        "# We transform them to Tensors of normalized range [-1, 1].\n",
        "\n",
        "transform_train = transforms.Compose(\n",
        "    [transforms.RandomHorizontalFlip(),\n",
        "     transforms.RandomCrop(32, padding=4),\n",
        "     transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "transform_test = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True, download=True, transform=transform_train)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=128, shuffle=True, num_workers=8, pin_memory=True)\n",
        "\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=transform_test)\n",
        "testloader = torch.utils.data.DataLoader(testset, batch_size=100, shuffle=False, num_workers=8, pin_memory=True)\n",
        "\n",
        "\n",
        "############################################\n",
        "# 2. Define a Convolutional Neural Network\n",
        "############################################\n",
        "\n",
        "#### kaiming initialization ####\n",
        "def He_init(layer):\n",
        "  if isinstance(layer, nn.Linear):\n",
        "      init.kaiming_normal_(layer.weight)\n",
        "  elif isinstance(layer, nn.Conv2d):\n",
        "      init.kaiming_normal_(layer.weight)\n",
        "  #elif isinstance(layer, nn.BatchNorm2d):\n",
        "  #    layer.weight.data.fill_(1)\n",
        "  #    layer.bias.data.zero_()\n",
        "\n",
        "#### Identitiy Mapping for matching dimension ####      \n",
        "class IdentityMapping(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride):\n",
        "      super(IdentityMapping, self).__init__()\n",
        "\t\t\n",
        "      self.pooling = nn.MaxPool2d(1, stride=stride)\n",
        "      self.dist_channels = out_channels - in_channels\n",
        "    \n",
        "    def forward(self, x):\n",
        "      x = F.pad(x, (0, 0, 0, 0, 0, self.dist_channels))\n",
        "      x = self.pooling(x)\n",
        "      return x\n",
        "\t\n",
        "#### each ResNet Block ####\n",
        "class ResNetBlock(nn.Module):\n",
        "    def __init__(self, in_channels, out_channels, stride=1, sub_sampling=False):\n",
        "      super(ResNetBlock, self).__init__()\n",
        "      \n",
        "      self.block_bn1 = nn.BatchNorm2d(in_channels)\n",
        "      self.block_conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3,\n",
        "                                  stride=stride, padding=1, bias=False)\n",
        "\n",
        "      self.block_bn2 = nn.BatchNorm2d(out_channels)\n",
        "      self.block_conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3,\n",
        "                                  stride=1, padding=1, bias=False)\n",
        "\n",
        "      \n",
        "      if sub_sampling:\n",
        "        self.sub_sampling = IdentityMapping(in_channels, out_channels, stride)\n",
        "      else:\n",
        "        self.sub_sampling = None\n",
        "      \n",
        "    def forward(self, x):\n",
        "      # make shortcut\n",
        "      shortcut = x\n",
        "      \n",
        "      # first layer of block\n",
        "      x = self.block_bn1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.block_conv1(x)\n",
        "      \n",
        "      # second layer of block\n",
        "      x = self.block_bn2(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.block_conv2(x)\n",
        "\n",
        "      # Identitiy Mapping\n",
        "      if self.sub_sampling is not None:\n",
        "        shortcut = self.sub_sampling(shortcut)\n",
        "\n",
        "      x += shortcut\n",
        "      #x = F.relu(x)\n",
        "      \n",
        "      return x\n",
        "\n",
        "#### Net Structure ####\n",
        "class Net(nn.Module):\n",
        "    # num_blocks : the number of each block.\n",
        "    # block : type of block (ResNetBlock)\n",
        "    def __init__(self, num_blocks, block):\n",
        "        super(Net, self).__init__()\n",
        "        \n",
        "        # input_size = 32 X 32 X 3 (W, H, C)\n",
        "        self.conv_1 = nn.Conv2d(3, 16, kernel_size=3, stride=1, padding=1, bias=False)\n",
        "        # output_size = 32 X 32 X 16 (W, H, C)\n",
        "        self.bn_1 = nn.BatchNorm2d(16)\n",
        "\n",
        "        # image_size = 32 X 32 X 16 (W, H, C)\n",
        "        self.block_1 = self.get_block(block, 16, 16, num_blocks, stride=1)\n",
        "        # image_size = 32 X 32 X 16 (W, H, C)\n",
        "        self.block_2 = self.get_block(block, 16, 32, num_blocks, stride=2)\n",
        "        # image_size = 16 X 16 X 32 (W, H, C)\n",
        "        self.block_3 = self.get_block(block, 32, 64, num_blocks, stride=2)\n",
        "        # image_size = 8 X 8 X 64 (W, H, C)\n",
        "        \n",
        "        self.bn = nn.BatchNorm2d(64)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "        \n",
        "        # input_size = 1 X 1 X 64\n",
        "        self.linear = nn.Linear(64, 10)\n",
        "        #end classification (matching 10 classes)\n",
        "        \n",
        "        self.apply(He_init)\n",
        "        \n",
        "    def get_block(self, block, in_channels, out_channels, num_blocks, stride):\n",
        "        layers = []\n",
        "      \n",
        "        if stride == 1:\n",
        "           sub_sampling = False\n",
        "        elif stride == 2:\n",
        "           sub_sampling = True\n",
        "          \n",
        "        layers.append(block(in_channels, out_channels, stride, sub_sampling))\n",
        "        \n",
        "        for i in range(num_blocks - 1):\n",
        "          layers.append(block(out_channels, out_channels, stride=1))\n",
        "          \n",
        "        return nn.Sequential(*layers)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        # first Convolution and Batch Normalization\n",
        "        x = self.conv_1(x)\n",
        "        x = self.bn_1(x)\n",
        "        x = F.relu(x)\n",
        "        \n",
        "        # block step\n",
        "        x = self.block_1(x)\n",
        "        x = self.block_2(x)\n",
        "        x = self.block_3(x)\n",
        "        \n",
        "        x = self.bn(x)\n",
        "        x = self.relu(x)\n",
        "        \n",
        "        # global_average_pooling and Fully-connected layer\n",
        "        # (8 X 8 X 64) image\n",
        "        x = F.avg_pool2d(x, 64)\n",
        "        # (1 X 1 X 64) Node\n",
        "        x = x.view(x.size(0), -1)\n",
        "        x = self.linear(x)\n",
        "        \n",
        "        return x\n",
        "\n",
        "# 6n+2 layer creates using Net(n, block)\n",
        "net = Net(3, ResNetBlock).to(device)\n",
        "\n",
        "############################################\n",
        "# 3. Define a Loss function and optimizer\n",
        "############################################\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(net.parameters(), lr=0.1, momentum=0.9, weight_decay=0.0001)\n",
        "#each 81, 123, 163 change the learning rate\n",
        "step_lr_scheduler = optim.lr_scheduler.MultiStepLR(optimizer, milestones=[150, 250], gamma=0.1)\n",
        "\n",
        "############################################\n",
        "# 4. Train the network\n",
        "############################################\n",
        "def train():\n",
        "    best_acc = 0\n",
        "    \n",
        "    for epoch in range(500):  # loop over the dataset multiple times\n",
        "\n",
        "        running_loss = 0.0\n",
        "        step_lr_scheduler.step()\n",
        "        net.train()\n",
        "        \n",
        "        for i, data in enumerate(trainloader):\n",
        "            # get the inputs\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            \n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            outputs = net(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            # print statistics\n",
        "            running_loss += loss.item()\n",
        "            if i % 100 == 99:  # print every 100 mini-batches\n",
        "                print('[{}, {}] loss: {:.4f}'.format(epoch + 1, i + 1, running_loss / 100))\n",
        "                running_loss = 0.0\n",
        "\n",
        "        print('epoch', epoch + 1, \" : \" , end=\" \")\n",
        "\n",
        "        # save the best model\n",
        "        test_acc = test()\n",
        "        if test_acc > best_acc:\n",
        "            best_acc = test_acc\n",
        "            best_model = copy.deepcopy(net)\n",
        "            \n",
        "            torch.save(best_model.state_dict(), 'gdrive/My Drive/best_model.pt')    \n",
        "    print('Finished Training')\n",
        "\n",
        "\n",
        "############################################\n",
        "# 5. Test the network on the test data\n",
        "############################################\n",
        "def test():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    accuracy = 0\n",
        "    net.eval()\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = net(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            accuracy = 100 * correct / total\n",
        "    print('Accuracy on test images: {:.2f}%'.format(accuracy))\n",
        "    return accuracy\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    train()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}